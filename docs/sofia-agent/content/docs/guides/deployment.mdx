---
title: Deployment
description: Learn how to deploy SOFIA agents in various environments including Docker, cloud services, and scaling considerations
---

# Deployment

This guide covers the different ways you can deploy SOFIA agents, from simple local deployments to complex cloud-based architectures. You'll learn about containerization, cloud deployment options, scaling strategies, and security best practices.

## Deployment Architecture

A typical SOFIA deployment includes these components:

<Mermaid
  chart="
    flowchart TB
      Client(Client Applications) <--> API[SOFIA API Server]
      API <--> Sofia[Sofia Agent]
      Sofia <--> LLM[LLM Provider]
      Sofia <--> Redis[(Redis Cache/Session Store)]
      Sofia <--> Tools[External Tools/APIs]
      API <--> Monitoring[Monitoring/Logging]"
/>

## Local Development Deployment

For development and testing, you can run SOFIA locally:

```python
from sofia import Sofia
from sofia.server import create_app

# Create a SOFIA agent
sofia = Sofia(
    name="my_agent",
    # Configuration...
)

# Create a FastAPI application
app = create_app(sofia)

# Run the server (for development only)
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

Run this with:

```bash
python app.py
```

## Docker Deployment

Containerizing SOFIA with Docker is recommended for production:

### Dockerfile

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose the port
EXPOSE 8000

# Set environment variables (override at runtime)
ENV SOFIA_LLM_PROVIDER=openai
ENV SOFIA_SESSION_STORE=redis

# Run the application with Gunicorn
CMD ["gunicorn", "app:app", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000"]
```

### Docker Compose

For a complete development environment:

```yaml
# docker-compose.yml
version: "3"

services:
  sofia-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - SOFIA_LLM_PROVIDER=openai
      - SOFIA_LLM_API_KEY=${OPENAI_API_KEY}
      - SOFIA_SESSION_STORE=redis
      - SOFIA_REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    volumes:
      - ./config:/app/config
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped

volumes:
  redis-data:
```

Run with:

```bash
docker-compose up -d
```

## Cloud Deployment

SOFIA can be deployed on various cloud platforms:

<Tabs>
  <Tab label="AWS">
    <Mermaid
      chart="
      flowchart TB
          ALB[AWS Load Balancer] --> ECS[ECS Fargate]
          ECS --> ECR[ECR Container Registry]
          ECS --> ElastiCache[ElastiCache Redis]
          ECS --> Secrets[Secrets Manager]
          ECS --> CloudWatch[CloudWatch Logs]"
    />
    
    ### AWS ECS Fargate

    1. Create an ECR repository for your Docker image
    2. Push your Docker image to ECR
    3. Set up an ECS cluster and service using Fargate
    4. Configure a load balancer and target group
    5. Deploy the service with appropriate environment variables

    ### Example CloudFormation Template (excerpt)

    ```yaml
    Resources:
      SofiaEcsCluster:
        Type: AWS::ECS::Cluster
        Properties:
          ClusterName: sofia-cluster

      SofiaTaskDefinition:
        Type: AWS::ECS::TaskDefinition
        Properties:
          Family: sofia-task
          Cpu: 1024
          Memory: 2048
          NetworkMode: awsvpc
          RequiresCompatibilities:
            - FARGATE
          ExecutionRoleArn: !GetAtt EcsExecutionRole.Arn
          TaskRoleArn: !GetAtt EcsTaskRole.Arn
          ContainerDefinitions:
            - Name: sofia-container
              Image: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/sofia:latest
              Essential: true
              PortMappings:
                - ContainerPort: 8000
              Environment:
                - Name: SOFIA_LLM_PROVIDER
                  Value: openai
                - Name: SOFIA_SESSION_STORE
                  Value: redis
                - Name: SOFIA_REDIS_URL
                  Value: !Sub redis://${SofiaElastiCache.RedisEndpoint.Address}:${SofiaElastiCache.RedisEndpoint.Port}/0
              LogConfiguration:
                LogDriver: awslogs
                Options:
                  awslogs-group: !Ref SofiaLogGroup
                  awslogs-region: !Ref AWS::Region
                  awslogs-stream-prefix: sofia
    ```

  </Tab>
  <Tab label="Azure">
    <Mermaid
      chart="
      flowchart TB
          FD[Azure Front Door] --> ACA[Azure Container Apps]
          ACA --> ACR[Azure Container Registry]
          ACA --> Redis[Azure Cache for Redis]
          ACA --> KeyVault[Azure Key Vault]
          ACA --> AppInsights[Application Insights]"
    />

    ### Azure Container Apps

    1. Create an Azure Container Registry
    2. Push your Docker image to ACR
    3. Deploy to Azure Container Apps
    4. Configure scaling rules based on HTTP traffic
    5. Set up Azure Cache for Redis for session storage

    ### Example Azure CLI Commands

    ```bash
    # Create a resource group
    az group create --name sofia-group --location eastus

    # Create Azure Container Registry
    az acr create --resource-group sofia-group --name sofiaregistry --sku Basic

    # Build and push to ACR
    az acr build --registry sofiaregistry --image sofia:latest .

    # Create Azure Container App Environment
    az containerapp env create \
      --name sofia-env \
      --resource-group sofia-group \
      --location eastus

    # Create the Container App
    az containerapp create \
      --name sofia-app \
      --resource-group sofia-group \
      --environment sofia-env \
      --image sofiaregistry.azurecr.io/sofia:latest \
      --target-port 8000 \
      --ingress external \
      --min-replicas 1 \
      --max-replicas 10 \
      --env-vars "SOFIA_LLM_PROVIDER=openai" "SOFIA_SESSION_STORE=redis"
    ```

  </Tab>
  <Tab label="GCP">
    <Mermaid
      chart="
      flowchart TB
          LB[Cloud Load Balancing] --> CR[Cloud Run]
          CR --> AR[Artifact Registry]
          CR --> MR[Memorystore Redis]
          CR --> SM[Secret Manager]
          CR --> Log[Cloud Logging]"
    />
    
    ### Google Cloud Run
    
    1. Push your Docker image to Google Artifact Registry
    2. Deploy to Cloud Run with appropriate environment variables
    3. Set up Memorystore (Redis) for session storage
    4. Configure Cloud Load Balancing for multiple regions
    
    ### Example gcloud Commands
    
    ```bash
    # Create a repository in Artifact Registry
    gcloud artifacts repositories create sofia-repo \
      --repository-format=docker \
      --location=us-central1
    
    # Build and push the image
    gcloud builds submit --tag us-central1-docker.pkg.dev/your-project/sofia-repo/sofia:latest
    
    # Deploy to Cloud Run
    gcloud run deploy sofia \
      --image=us-central1-docker.pkg.dev/your-project/sofia-repo/sofia:latest \
      --platform=managed \
      --region=us-central1 \
      --allow-unauthenticated \
      --memory=2Gi \
      --cpu=1 \
      --min-instances=1 \
      --max-instances=10 \
      --set-env-vars="SOFIA_LLM_PROVIDER=openai,SOFIA_SESSION_STORE=redis,SOFIA_REDIS_URL=redis://10.0.0.1:6379"
    ```
  </Tab>
</Tabs>

## Kubernetes Deployment

For more complex deployments, Kubernetes offers flexibility and scalability:

### Deployment Manifest

```yaml
# sofia-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sofia
  labels:
    app: sofia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sofia
  template:
    metadata:
      labels:
        app: sofia
    spec:
      containers:
        - name: sofia
          image: your-registry/sofia:latest
          ports:
            - containerPort: 8000
          env:
            - name: SOFIA_LLM_PROVIDER
              value: "openai"
            - name: SOFIA_LLM_API_KEY
              valueFrom:
                secretKeyRef:
                  name: sofia-secrets
                  key: openai-api-key
            - name: SOFIA_SESSION_STORE
              value: "redis"
            - name: SOFIA_REDIS_URL
              value: "redis://redis-service:6379/0"
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 15
            periodSeconds: 10
```

### Service Manifest

```yaml
# sofia-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: sofia-service
spec:
  selector:
    app: sofia
  ports:
    - port: 80
      targetPort: 8000
  type: ClusterIP
```

### Ingress Manifest

```yaml
# sofia-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: sofia-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: sofia.yourdomain.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: sofia-service
                port:
                  number: 80
  tls:
    - hosts:
        - sofia.yourdomain.com
      secretName: sofia-tls-secret
```

Apply with:

```bash
kubectl apply -f sofia-deployment.yaml
kubectl apply -f sofia-service.yaml
kubectl apply -f sofia-ingress.yaml
```

## Scaling Considerations

### Horizontal Scaling

SOFIA can be horizontally scaled by:

1. **Stateless API Servers**: Deploy multiple instances behind a load balancer
2. **Session Persistence**: Use Redis for session storage to share state
3. **Auto-scaling**: Configure auto-scaling based on CPU, memory, or request rate

Example auto-scaling configuration for Kubernetes:

```yaml
# sofia-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: sofia-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sofia
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
```

### Vertical Scaling

For memory-intensive workloads:

1. Increase container memory and CPU allocations
2. Choose appropriate instance types (more RAM for complex flows)
3. Monitor memory usage to avoid out-of-memory errors

### Scaling Redis

As your SOFIA deployment grows:

1. Start with a single Redis instance
2. Move to Redis replication for read scalability
3. Consider Redis Cluster for large-scale deployments
4. Implement proper backup strategies

## Security Best Practices

### API Security

1. **Authentication**: Implement API keys or OAuth 2.0
2. **Rate Limiting**: Protect against abuse
3. **Input Validation**: Validate all user inputs

Example FastAPI authentication:

```python
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import APIKeyHeader

app = FastAPI()
api_key_header = APIKeyHeader(name="X-API-Key")

def get_api_key(api_key: str = Depends(api_key_header)):
    if api_key != os.getenv("SOFIA_API_KEY"):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API Key"
        )
    return api_key

@app.post("/chat", dependencies=[Depends(get_api_key)])
async def chat(request: ChatRequest):
    # Process chat request
    pass
```

### Secret Management

Never hardcode secrets. Use:

1. Environment variables
2. Cloud secret management services:
   - AWS Secrets Manager
   - Azure Key Vault
   - Google Secret Manager
3. Kubernetes Secrets

Example Kubernetes secret:

```yaml
# sofia-secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: sofia-secrets
type: Opaque
data:
  openai-api-key: base64-encoded-api-key
  redis-password: base64-encoded-redis-password
```

### Network Security

1. Implement proper network policies
2. Use HTTPS for all communications
3. Consider a WAF (Web Application Firewall)

Example Kubernetes network policy:

```yaml
# sofia-network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: sofia-network-policy
spec:
  podSelector:
    matchLabels:
      app: sofia
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: frontend
      ports:
        - protocol: TCP
          port: 8000
  egress:
    - to:
        - podSelector:
            matchLabels:
              app: redis
      ports:
        - protocol: TCP
          port: 6379
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              - 10.0.0.0/8
              - 172.16.0.0/12
              - 192.168.0.0/16
      ports:
        - protocol: TCP
          port: 443
```

## Monitoring and Observability

### Logging

Configure comprehensive logging:

```python
import logging
from sofia import Sofia

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("sofia.log")
    ]
)

# Create SOFIA agent with logging
sofia = Sofia(
    name="my_agent",
    enable_logging=True,
    log_level="INFO",
    # Other configuration...
)
```

### Health Checks

Implement health check endpoints:

```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/health")
async def health_check():
    # Check Redis connection
    redis_status = check_redis_connection()

    # Check LLM provider connection
    llm_status = check_llm_connection()

    if redis_status and llm_status:
        return {"status": "healthy"}
    else:
        return {"status": "unhealthy", "details": {
            "redis": redis_status,
            "llm": llm_status
        }}
```

### Metrics

Expose metrics for monitoring:

```python
from prometheus_client import Counter, Histogram, start_http_server
import time

# Metrics
request_count = Counter('sofia_request_total', 'Total number of requests')
response_time = Histogram('sofia_response_time_seconds', 'Response time in seconds')

# Start Prometheus metrics server
start_http_server(8001)

# Instrument your application
@app.post("/chat")
async def chat(request: ChatRequest):
    request_count.inc()

    start_time = time.time()
    response = await process_chat(request)
    duration = time.time() - start_time

    response_time.observe(duration)
    return response
```

## CI/CD Pipeline

Implement a CI/CD pipeline for automated deployment:

<Mermaid
  chart="
    graph LR
      Code[Code Repository] --> Build[Build & Test]
      Build --> Publish[Publish Image]
      Publish --> Deploy[Deploy to Staging]
      Deploy --> Test[Integration Tests]
      Test --> Prod[Deploy to Production]"
/>

Example GitHub Actions workflow:

```yaml
# .github/workflows/deploy.yml
name: Deploy SOFIA

on:
  push:
    branches: [main]

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest

      - name: Test
        run: |
          pytest

      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: your-registry/sofia:latest

      - name: Deploy to Kubernetes
        uses: Azure/k8s-deploy@v1
        with:
          manifests: |
            k8s/sofia-deployment.yaml
            k8s/sofia-service.yaml
            k8s/sofia-ingress.yaml
          images: your-registry/sofia:latest
          namespace: default
```

## Multi-region Deployment

For global applications, consider multi-region deployment:

<Mermaid
  chart='
  flowchart TB
    GlobalLB[Global Load Balancer] --> RegionA[Region A]
    GlobalLB --> RegionB[Region B]
    GlobalLB --> RegionC[Region C]
    
    subgraph "Region A"
    LoadBalancerA[Load Balancer] --> DeploymentA[SOFIA Deployment]
    DeploymentA --> RedisA[Redis]
    end
    
    subgraph "Region B"
    LoadBalancerB[Load Balancer] --> DeploymentB[SOFIA Deployment]
    DeploymentB --> RedisB[Redis]
    end
    
    subgraph "Region C"
    LoadBalancerC[Load Balancer] --> DeploymentC[SOFIA Deployment]
    DeploymentC --> RedisC[Redis]
    end'
/>

Benefits:

- Lower latency for users worldwide
- Greater resilience to regional outages
- Compliance with data residency requirements

Challenges:

- Session consistency across regions
- Increased operational complexity
- Higher costs

## Deployment Best Practices

1. **Start Simple**: Begin with a single-instance deployment and scale as needed
2. **Infrastructure as Code**: Define your infrastructure using IaC tools like Terraform
3. **Blue-Green Deployments**: Minimize downtime with staged deployments
4. **Monitoring**: Implement comprehensive monitoring from day one
5. **Backup and Recovery**: Regular backups of all stateful components
6. **Documentation**: Keep deployment documentation updated
7. **Load Testing**: Test your deployment under expected load conditions

<Callout type="info">
  For large-scale deployments, consider implementing circuit breakers and
  fallback strategies for LLM API calls to prevent cascading failures.
</Callout>

## Next Steps

- Learn about [Monitoring and Debugging](/guides/monitoring) to monitor your deployed SOFIA agents
- Explore [Configuration](/guides/configuration) to fine-tune your deployment
- Visit [Extending SOFIA](/guides/extending-sofia) to learn how to extend SOFIA with custom components
